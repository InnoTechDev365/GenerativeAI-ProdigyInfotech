# ðŸ§  Text Generation using GPT-2  
**Internship Assignment | Prodigy InfoTech - PRODIGY_GA_01**

This project demonstrates how to use the GPT-2 language model for generating coherent and contextually relevant text from a given prompt. It uses Hugging Face's `transformers` library to load and run the pre-trained GPT-2 model locally.

---

## ðŸš€ Features

- Uses official **GPT-2** model (`gpt2`)
- Generates human-like text using **beam search**
- Prevents repetition using `no_repeat_ngram_size`
- Adjustable prompt and output length

---

## â–¶ï¸ How to Run

### Option 1: Google Colab (Recommended)

1. Go to [Google Colab](https://colab.research.google.com/ )
2. Click **File > Upload Notebook**
3. Upload `Task01_GenAI.ipynb`
4. Click **Runtime > Run All**

### Option 2: Local Machine

1. Install Jupyter Notebook:
   ```bash
   pip install notebook
